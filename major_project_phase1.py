# -*- coding: utf-8 -*-
"""Major_project_Phase1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nMso1EI5eXI-xB7AVxXxEsDVTbrmuSgm

# Basic pipeline of RAG

# Document Collection & Organization
"""

!pip install -q langchain langchain-community langchain-openai sentence-transformers faiss-cpu

!pip install -q pypdf

# Step A1: Mount Google Drive if needed
from google.colab import drive
drive.mount('/content/drive')

# Step A2: Define path to your document
pdf_path = "/content/12_homescience_eng_sm_2024.pdf"  # adjust

# Step A3: Extract text & metadata
from langchain_community.document_loaders import PyPDFLoader
from datetime import datetime

loader = PyPDFLoader(pdf_path)
documents = loader.load()

# Attach simple metadata
for d in documents:
    d.metadata["subject"] = "Food Safety & Nutrition"
    d.metadata["source"] = "NCERT Textbook"
    d.metadata["timestamp"] = datetime.now().isoformat()

from langchain.text_splitter import RecursiveCharacterTextSplitter

# Step B1: Define chunking strategy
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,    # adjustable
    chunk_overlap=200,  # ensures semantic continuity
    length_function=len,
)

chunks = splitter.split_documents(documents)
print(f"Total chunks: {len(chunks)}")

# Step B2: (Optional) Preprocessing
def clean_text(t):
    import re
    t = t.lower()
    t = re.sub(r'\s+', ' ', t)   # collapse spaces
    t = re.sub(r'[^a-z0-9.,;:?!()\s-]', '', t)  # basic cleanup
    return t

for c in chunks:
    c.page_content = clean_text(c.page_content)

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle

model = SentenceTransformer("all-mpnet-base-v2")
embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)

# Create FAISS index
dim = embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(embeddings)

# Save manually
faiss.write_index(index, "faiss_food_safety.index")
with open("metadata.pkl", "wb") as f:
    pickle.dump((texts, metadatas), f)

# from langchain.chains import RetrievalQA
# from langchain_openai import ChatOpenAI

# # Initialize OpenAI (use your Colab secret)
# import os
# os.environ["OPENAI_API_KEY"] = "sk-..."  # replace with your key

# # Step D1: Load FAISS index
# from langchain.vectorstores import FAISS
# from langchain_openai import OpenAIEmbeddings

# retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})

# # Step D2: Build QA chain
# llm = ChatOpenAI(model="gpt-4o-mini")  # fast, affordable
# qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# # Step D3: Query
# query = "What are the key methods of food preservation?"
# answer = qa.run(query)

# print("Q:", query)
# print("A:", answer)

import faiss, pickle
import numpy as np
from sentence_transformers import SentenceTransformer

# Paths in your Google Drive
faiss_path = "/content/drive/MyDrive/fsn_outputs/faiss_food_safety.index"
meta_path  = "/content/drive/MyDrive/fsn_outputs/metadata.pkl"

# Load FAISS index
index = faiss.read_index(faiss_path)

# Load texts and metadata
with open(meta_path, "rb") as f:
    texts, metadatas = pickle.load(f)

# Re-load embedding model
model = SentenceTransformer("all-mpnet-base-v2")

print(f"Loaded {len(texts)} chunks from {meta_path}")

def retrieve(query, top_k=4):
    q_emb = model.encode([query], convert_to_numpy=True)
    D, I = index.search(q_emb, top_k)
    results = []
    for score, idx in zip(D[0], I[0]):
        results.append((texts[idx], metadatas[idx], float(score)))
    return results

query = "What are the main causes of food spoilage?"
results = retrieve(query)
for i, (chunk, meta, score) in enumerate(results, 1):
    print(f"\n--- Result {i} (score={score:.2f}) ---\n{chunk[:400]}...")

from langchain_openai import ChatOpenAI
import os

os.environ["OPENAI_API_KEY"] = "..."  # üîë put your key here
llm = ChatOpenAI(model="gpt-4o-mini")

def answer_query(query, top_k=4):
    retrieved = retrieve(query, top_k)
    context = "\n\n".join([c for c, _, _ in retrieved])
    prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer concisely and clearly:"
    response = llm.invoke(prompt)
    return response.content

print(answer_query("Explain the different methods of food preservation."))

"""# exploring the bonus parts and modifying the above basic pipeline

## A. Document Collection & Organization
* All source documents must be collected in a root-level directory.
* The system must support heterogeneous formats, including PDF,
DOCX, PPTX, TXT, and MD. Web-scraped text files are also
encouraged.
* Each document must be associated with metadata (e.g., subject,
source, context, timestamp).
* The pipeline should automatically detect types and process
documents.
* There is no data volume which can be suggested for the project, it
would vary depending on your chosen domain. You need to use your
judgement and decide. Collect data you feel is optimal and justify your
decision.
* Guideline: Typically, every subject/domain should cover most important text books and other authoritative content without duplication.
In addition, there can be an equivalent amount of other relevant/supplementary content.
"""

# Step A1: Mount Google Drive (root-level directory for all sources)
from google.colab import drive
drive.mount('/content/drive')

import os
from datetime import datetime
from langchain_community.document_loaders import (
    PyPDFLoader, UnstructuredWordDocumentLoader,
    UnstructuredPowerPointLoader, TextLoader, UnstructuredMarkdownLoader
)

# Step A2: Define root directory for all documents
root_dir = "/content/drive/MyDrive/food_safety_nutrition_corpus/LMA_MAJOR_PROJECT_RAW-PDFS"  # ‚Üê create & store all source files here
documents = []

# Step A3: Automatically detect and process each supported file type
for fname in os.listdir(root_dir):
    fpath = os.path.join(root_dir, fname)
    if not os.path.isfile(fpath):
        continue

    ext = fname.lower().split(".")[-1]
    try:
        if ext == "pdf":
            loader = PyPDFLoader(fpath)
        elif ext in ["doc", "docx"]:
            loader = UnstructuredWordDocumentLoader(fpath)
        elif ext in ["ppt", "pptx"]:
            loader = UnstructuredPowerPointLoader(fpath)
        elif ext == "txt":
            loader = TextLoader(fpath)
        elif ext in ["md", "markdown"]:
            loader = UnstructuredMarkdownLoader(fpath)
        else:
            print(f"‚ö†Ô∏è Skipping unsupported file type: {fname}")
            continue

        # Load and append documents
        loaded_docs = loader.load()
        for d in loaded_docs:
            # Attach rich metadata
            d.metadata["subject"]   = "Food Safety & Nutrition"
            d.metadata["source"]    = fname
            d.metadata["context"]   = "Authoritative / Supplementary reference"
            d.metadata["timestamp"] = datetime.now().isoformat()
            d.metadata["file_type"] = ext
            documents.append(d)

    except Exception as e:
        print(f"‚ùå Error processing {fname}: {e}")

print(f"\n‚úÖ Total documents loaded: {len(documents)}")
print(f"Sample metadata:\n{documents[0].metadata if documents else 'No documents loaded'}")

"""# B. Preprocessing & Chunking
* Decide on the right chunking strategy. Think about various possibilities such as, fixed-size, content-aware (e.g., paragraph-based), and recursive character splitting and decide what you think is the best option for your domain. Justify your decision in the documentation.

* Segment documents at multiple granularities (e.g., 2048, 512, 128 tokens) to facilitate hierarchical retrieval.

* Implement a context-aware overlap strategy to ensure semantic continuity.

* Preprocessing must include deduplication, tokenization, lowercasing, and removal of non-informative content.

* [BONUS] Develop an automated batch ingestion pipeline with
comprehensive error logging.


"""

# =========================
# B. PREPROCESSING & CHUNKING
# =========================
# What this cell does:
# 1) Preprocess (deduplicate, lowercase, denoise) all Documents
# 2) Chunk using TWO strategies:
#    (a) Content-aware: RecursiveCharacterTextSplitter (paragraph/sentence aware)
#    (b) Hierarchical token chunks at 2048 / 512 / 128 tokens (with overlap)  <-- REQUIRED
# 3) Bonus: batch ingestion + robust logging

import re, html, hashlib, logging, os, math
from typing import List, Dict, Iterable
from datetime import datetime
from tqdm.auto import tqdm

from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

from transformers import AutoTokenizer

# -------------------------
# Logging setup (BONUS)
# -------------------------
os.makedirs("/content/logs", exist_ok=True)
LOG_PATH = "/content/logs/ingestion.log"
logging.basicConfig(
    filename=LOG_PATH,
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
)
logging.info("=== Ingestion run started ===")

# -------------------------
# Preprocessing utilities
# -------------------------
_URL = re.compile(r"https?://\S+|www\.\S+")
_HTMLTAG = re.compile(r"<[^>]+>")
_NON_INFO = re.compile(r"[^a-z0-9.,;:?!()\s-]+")  # keep light punctuation
_MULTI_WS = re.compile(r"\s+")

def normalize_text(t: str) -> str:
    """Lowercase, strip URLs/HTML, remove non-informative chars, collapse spaces."""
    t = t or ""
    t = html.unescape(t)
    t = t.lower()
    t = _URL.sub(" ", t)
    t = _HTMLTAG.sub(" ", t)
    t = _NON_INFO.sub(" ", t)
    t = _MULTI_WS.sub(" ", t).strip()
    return t

def is_informative(t: str, min_alpha=30) -> bool:
    """Filter out empty/very short chunks after cleaning."""
    # count letters/digits as proxy for information
    return sum(ch.isalnum() for ch in t) >= min_alpha

def dedup_documents(docs: Iterable[Document]) -> List[Document]:
    """Exact dedup on normalized text hash (simple + fast)."""
    seen = set()
    unique = []
    for d in docs:
        norm = normalize_text(d.page_content)
        h = hashlib.md5(norm.encode("utf-8")).hexdigest()
        if h in seen:
            continue
        seen.add(h)
        if is_informative(norm):
            d.page_content = norm  # keep normalized content
            unique.append(d)
    return unique

# -------------------------
# Chunking strategies
# -------------------------

# (1) CONTENT-AWARE CHUNKING (used for base splits)
#     We use RecursiveCharacterTextSplitter to respect paragraph/sentence-like boundaries.
content_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,     # content-aware size in characters
    chunk_overlap=200,   # ~20% overlap for semantic continuity
    length_function=len
)

# (2) HIERARCHICAL TOKEN CHUNKING (USED TO SATISFY MULTI-GRANULARITY REQUIREMENT)
#     Token-based splits at 2048 / 512 / 128 tokens with ~15% overlap each.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def chunk_by_tokens(text: str, size: int, overlap: int) -> List[str]:
    """Split text into token windows with overlap using a HF tokenizer."""
    if not text:
        return []
    ids = tokenizer.encode(text, add_special_tokens=False)
    chunks = []
    if len(ids) == 0:
        return chunks
    start = 0
    while start < len(ids):
        end = min(start + size, len(ids))
        piece_ids = ids[start:end]
        chunk = tokenizer.decode(piece_ids, clean_up_tokenization_spaces=True)
        if is_informative(chunk):
            chunks.append(chunk)
        if end == len(ids):
            break
        start = max(0, end - overlap)  # slide with overlap
    return chunks

def hierarchical_token_chunks(doc: Document) -> Dict[str, List[Document]]:
    """Create 2048 / 512 / 128-token chunks with context-aware overlap (~15%)."""
    text = doc.page_content
    # choose overlap proportional to size (~15%)
    specs = [
        ("lvl_2048", 2048, int(0.15 * 2048)),
        ("lvl_0512",  512, int(0.15 *  512)),
        ("lvl_0128",  128, int(0.15 *  128)),
    ]
    out = {}
    for label, size, ov in specs:
        parts = chunk_by_tokens(text, size=size, overlap=ov)
        out[label] = [
            Document(
                page_content=p,
                metadata={**doc.metadata, "granularity": label, "chunk_size_tokens": size, "overlap_tokens": ov}
            )
            for p in parts
        ]
    return out

# -------------------------
# Batch pipeline (BONUS)
# -------------------------
def process_documents_batch(docs: List[Document], batch_size: int = 200):
    """
    Automated batch ingestion with robust error logging.
    Steps:
      1) Preprocess: lowercase, denoise, deduplicate (exact)
      2) Content-aware base chunks
      3) Multi-granularity token chunks (2048/512/128) with overlap (‚âà15%)
    Returns:
      dict with lists for each level + combined union
    """
    logging.info(f"Received {len(docs)} raw documents")

    # 1) Preprocess + deduplicate
    try:
        pre = dedup_documents(docs)
        logging.info(f"After dedup & cleaning: {len(pre)} documents")
    except Exception as e:
        logging.exception(f"Preprocessing failed: {e}")
        pre = []

    # 2) Content-aware base split (useful as first-pass retriever or fallback)
    base_chunks: List[Document] = []
    for i in tqdm(range(0, len(pre), batch_size), desc="Content-aware splitting"):
        batch = pre[i:i+batch_size]
        try:
            split_batch = content_splitter.split_documents(batch)
            # add granularity metadata for traceability
            for d in split_batch:
                d.metadata["granularity"] = "content_recursive_1000c_200c_ovl"
                d.metadata["chunk_size_chars"] = 1000
                d.metadata["overlap_chars"] = 200
            base_chunks.extend(split_batch)
        except Exception as e:
            logging.exception(f"Split error on batch {i}:{i+batch_size} - {e}")

    # 3) Hierarchical token-based splits
    lvl_2048, lvl_512, lvl_128 = [], [], []
    for d in tqdm(pre, desc="Token-based hierarchical splits"):
        try:
            levels = hierarchical_token_chunks(d)
            lvl_2048.extend(levels["lvl_2048"])
            lvl_512.extend(levels["lvl_0512"])
            lvl_128.extend(levels["lvl_0128"])
        except Exception as e:
            logging.exception(f"Token split failed for doc source={d.metadata.get('source','?')} | {e}")

    # Final small clean-up: drop any short/non-informative residuals
    def _final_filter(xs: List[Document]) -> List[Document]:
        out = []
        for d in xs:
            if is_informative(d.page_content):
                out.append(d)
        return out

    base_chunks = _final_filter(base_chunks)
    lvl_2048   = _final_filter(lvl_2048)
    lvl_512    = _final_filter(lvl_512)
    lvl_128    = _final_filter(lvl_128)

    # Stats
    logging.info(f"Base content-aware chunks: {len(base_chunks)}")
    logging.info(f"2048-token chunks: {len(lvl_2048)}")
    logging.info(f" 512-token chunks: {len(lvl_512)}")
    logging.info(f" 128-token chunks: {len(lvl_128)}")

    return {
        "content_recursive": base_chunks,
        "lvl_2048": lvl_2048,
        "lvl_512":  lvl_512,
        "lvl_128":  lvl_128,
        "log_path": LOG_PATH
    }

# ======= RUN PIPELINE =======
# Expecting `documents` from Section A (PDF + HF already appended)
result = process_documents_batch(documents, batch_size=200)

print("‚úÖ Done. Chunk counts:")
for k in ["content_recursive", "lvl_2048", "lvl_512", "lvl_128"]:
    print(f"  {k}: {len(result[k])}")
# print(f"‚ÑπÔ∏è Logs written to: {result['log_path']}")

# =========================
# FIXED LOGGING CONFIG
# =========================
import logging, os

os.makedirs("/content/logs", exist_ok=True)
LOG_PATH = "/content/logs/ingestion.txt"  # changed from .log ‚Üí .txt

logging.basicConfig(
    filename=LOG_PATH,
    level=logging.INFO,
    filemode="w",  # overwrite each run for cleanliness
    format="%(asctime)s | %(levelname)s | %(message)s",
    force=True      # ensure config reloads if run multiple times
)

logging.info("=== Ingestion run started ===")

# ---- after the process_documents_batch() call ----
result = process_documents_batch(documents, batch_size=200)

print("‚úÖ Done. Chunk counts:")
for k in ["content_recursive", "lvl_2048", "lvl_512", "lvl_128"]:
    print(f"  {k}: {len(result[k])}")

# Explicitly flush and close log handlers (ensures file is visible in Colab)
for handler in logging.root.handlers[:]:
    handler.flush()
    handler.close()

print(f"üìÑ Logs saved to: {LOG_PATH}")

# Optional: display the last few lines of log to verify
!tail -n 10 /content/logs/ingestion.txt

"""# C. Embedding & Indexing
* For generating embeddings, teams might consider using libraries like
sentence-transformers (with a model like all-mpnet-base-v2 as a
starting point). You are encouraged to experiment with and evaluate at least one other domain-specific embedding model.

* Store embeddings with parent-child relationships to maintain document
structure.

* Chunks should be indexed into a vector database. Options to explore
include Elasticsearch, Pinecone, or Milvus.

* [BONUS] Implement a reranking step (e.g., using BGE Reranker) with a fallback to basic similarity search.
"""

# ====== COMMON HELPERS ======
import os, uuid, pickle, json
from typing import List, Dict
import numpy as np
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer

# Input: the dict produced earlier by Section B
# result = {... "content_recursive": List[Document], "lvl_2048": [...], ...}

# Build unified list of CHUNK docs + a set of PARENT docs (by (source,file))
def build_parent_child(result_dict) -> Dict[str, List[dict]]:
    """
    Returns:
      parents:  list[ {parent_id, source, subject, file_type, extra_meta...} ]
      children: list[ {chunk_id, parent_id, text, meta} ]
    """
    parents = {}  # key: (source) -> parent_id + meta
    children = []

    # merge all granularities
    all_chunks = []
    for k in ["content_recursive", "lvl_2048", "lvl_512", "lvl_128"]:
        all_chunks.extend(result_dict.get(k, []))

    for d in all_chunks:
        meta = dict(d.metadata or {})
        source = meta.get("source", "unknown_source")
        # register parent if new
        if source not in parents:
            parent_id = f"parent::{uuid.uuid4().hex}"
            parents[source] = {
                "parent_id": parent_id,
                "source": source,
                "subject": meta.get("subject", "Food Safety & Nutrition"),
                "file_type": meta.get("file_type", "txt"),
                "timestamp": meta.get("timestamp"),
                "context": meta.get("context", "Authoritative / Supplementary reference")
            }
        parent_id = parents[source]["parent_id"]

        # make child
        children.append({
            "chunk_id": f"chunk::{uuid.uuid4().hex}",
            "parent_id": parent_id,
            "text": d.page_content,
            "meta": meta
        })

    return {
        "parents": list(parents.values()),
        "children": children
    }

bundle = build_parent_child(result)
len(bundle["parents"]), len(bundle["children"])

"""# general embeddings"""

# ====== GENERAL EMBEDDINGS (Baseline) ======
GENERAL_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
general_model = SentenceTransformer(GENERAL_MODEL_NAME)

def embed_texts_general(texts: List[str]) -> np.ndarray:
    return general_model.encode(texts, show_progress_bar=True, convert_to_numpy=True)

"""# Domain-specific embeddings (biomedical/food-safety)"""

# ====== DOMAIN-SPECIFIC EMBEDDINGS (Biomedical/Food) ======
DOMAIN_MODEL_NAME = "pritamdeka/S-Biomed-Roberta-snli-multinli-stsb"
domain_model = SentenceTransformer(DOMAIN_MODEL_NAME)

def embed_texts_domain(texts: List[str]) -> np.ndarray:
    return domain_model.encode(texts, show_progress_bar=True, convert_to_numpy=True)

# ====== FAISS: BUILD (GENERAL) ======
import faiss

faiss_out_dir = "/content/faiss_general"
os.makedirs(faiss_out_dir, exist_ok=True)

texts = [c["text"] for c in bundle["children"]]
metas = [c for c in bundle["children"]]  # keep full dict with parent_id

gen_emb = embed_texts_general(texts)
dim = gen_emb.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(gen_emb)

faiss.write_index(index, os.path.join(faiss_out_dir, "index.faiss"))
with open(os.path.join(faiss_out_dir, "meta.pkl"), "wb") as f:
    pickle.dump({"parents": bundle["parents"], "children": metas}, f)

print("‚úÖ FAISS (general) built:", faiss_out_dir)

# ====== FAST FAISS: BUILD (GENERAL) ======
import os, pickle, faiss, torch
import numpy as np
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer

# Output directory
faiss_out_dir = "/content/faiss_general"
os.makedirs(faiss_out_dir, exist_ok=True)

# 1Ô∏è‚É£ Choose fast model & device
device = "cuda" if torch.cuda.is_available() else "cpu"
GENERAL_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"  # baseline
# Fallback small model for CPU-only mode
if device == "cpu":
    GENERAL_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

print(f"‚öôÔ∏è Using model: {GENERAL_MODEL_NAME} on device: {device}")
model = SentenceTransformer(GENERAL_MODEL_NAME, device=device)

# 2Ô∏è‚É£ Prepare text chunks
texts = [c["text"] for c in bundle["children"]]
metas = [c for c in bundle["children"]]
print(f"üìö Total chunks: {len(texts)}")

# 3Ô∏è‚É£ Cache embeddings to avoid recomputation
cache_path = os.path.join(faiss_out_dir, "gen_emb.npy")
if os.path.exists(cache_path):
    print("üì¶ Loading cached embeddings from disk...")
    gen_emb = np.load(cache_path)
else:
    print("üß† Computing embeddings in batches...")
    batch_size = 64 if device == "cuda" else 16
    all_vecs = []
    for i in tqdm(range(0, len(texts), batch_size)):
        batch = texts[i:i + batch_size]
        emb = model.encode(batch, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
        all_vecs.append(emb)
    gen_emb = np.vstack(all_vecs)
    np.save(cache_path, gen_emb)
    print(f"‚úÖ Embeddings computed and saved to {cache_path}")

# 4Ô∏è‚É£ Build FAISS index
dim = gen_emb.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(gen_emb)

faiss.write_index(index, os.path.join(faiss_out_dir, "index.faiss"))
with open(os.path.join(faiss_out_dir, "meta.pkl"), "wb") as f:
    pickle.dump({"parents": bundle["parents"], "children": metas}, f)

print(f"‚úÖ FAISS index built and saved at: {faiss_out_dir}")
print(f"üìè Embedding dimension: {dim}")
print(f"üíæ Metadata + embeddings stored. Ready for retrieval!")

cp -r /content/faiss_general  /content/drive/MyDrive/fsn_outputs/

cp -r /content/logs /content/drive/MyDrive/fsn_outputs/

# ====== FAISS: BUILD (DOMAIN-SPECIFIC) ======
faiss_out_dir_ds = "/content/faiss_domain"
os.makedirs(faiss_out_dir_ds, exist_ok=True)

dom_emb = embed_texts_domain(texts)
dim2 = dom_emb.shape[1]
index2 = faiss.IndexFlatL2(dim2)
index2.add(dom_emb)

faiss.write_index(index2, os.path.join(faiss_out_dir_ds, "index.faiss"))
with open(os.path.join(faiss_out_dir_ds, "meta.pkl"), "wb") as f:
    pickle.dump({"parents": bundle["parents"], "children": metas}, f)

print("‚úÖ FAISS (domain) built:", faiss_out_dir_ds)

cp -r /content/faiss_domain  /content/drive/MyDrive/fsn_outputs/

"""# Query FAISS (+ BONUS reranking)"""

# Load the domain FAISS index you built earlier
import faiss, pickle, os

faiss_domain_dir = "/content/drive/MyDrive/fsn_outputs/faiss_domain"

index2 = faiss.read_index(os.path.join(faiss_domain_dir, "index.faiss"))
with open(os.path.join(faiss_domain_dir, "meta.pkl"), "rb") as f:
    meta_domain = pickle.load(f)

# use its children metadata for domain retrieval
metas = meta_domain["children"]
print(f"‚úÖ Domain FAISS index loaded with {index2.ntotal} vectors (dim={index2.d})")

# ====== RETRIEVAL (FAISS) with optional BGE RERANKER ======
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import List
import numpy as np
from sentence_transformers import SentenceTransformer # Import SentenceTransformer

# -------------------------
# 1Ô∏è‚É£ Load Reranker (BGE Base)
# -------------------------
try:
    RERANKER_MODEL = "BAAI/bge-reranker-base"
    rr_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL)
    rr_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL)
    rr_model.eval()
    rr_device = "cuda" if torch.cuda.is_available() else "cpu"
    rr_model.to(rr_device)
    use_reranker = True
    print(f"‚úÖ Reranker loaded on {rr_device}: {RERANKER_MODEL}")
except Exception as e:
    print("‚ö†Ô∏è Could not load reranker, using FAISS similarity only.")
    use_reranker = False

# -------------------------
# 2Ô∏è‚É£ Define Reranking Function
# -------------------------
def rerank_bge(query: str, candidates: List[dict], top_k: int = 5) -> List[dict]:
    """Apply BGE reranker or fallback to FAISS scores if unavailable."""
    if not use_reranker:
        return candidates[:top_k]

    pairs = [(query, c["text"]) for c in candidates]
    inputs = rr_tokenizer.batch_encode_plus(
        pairs, padding=True, truncation=True, return_tensors="pt", max_length=512
    ).to(rr_device)

    with torch.no_grad():
        scores = rr_model(**inputs).logits.squeeze(-1).cpu().numpy()

    # Attach rerank scores and normalize
    for c, s in zip(candidates, scores.tolist()):
        c["rerank_score"] = float(s)
    ranked = sorted(candidates, key=lambda x: x["rerank_score"], reverse=True)
    return ranked[:top_k]

# -------------------------
# Load Embedding Models (outside the function)
# -------------------------
GENERAL_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2" # smaller model for faster testing
DOMAIN_MODEL_NAME = "pritamdeka/S-Biomed-Roberta-snli-multinli-stsb"

general_model = SentenceTransformer(GENERAL_MODEL_NAME)
domain_model = SentenceTransformer(DOMAIN_MODEL_NAME)
print("‚úÖ Embedding models loaded.")


# -------------------------
# 3Ô∏è‚É£ Unified FAISS Search (General / Domain)
# -------------------------

def faiss_search(query: str, top_k: int = 5, domain: bool = False):
    """
    Retrieves from the correct FAISS index (general or domain)
    and applies BGE reranker if available.
    """

    if domain:
        # ----- Domain search -----
        idx_path = "/content/drive/MyDrive/fsn_outputs/faiss_domain/index.faiss"
        meta_path = "/content/drive/MyDrive/fsn_outputs/faiss_domain/meta.pkl"
        model = domain_model # Use domain_model for domain search
    else:
        # ----- General search -----
        idx_path = "/content/drive/MyDrive/fsn_outputs/faiss_general/index.faiss"
        meta_path = "/content/drive/MyDrive/fsn_outputs/faiss_general/meta.pkl"
        model = general_model # Use general_model for general search

    # Load FAISS + metadata fresh each call (lightweight)
    import faiss, pickle, os # Import os
    index_local = faiss.read_index(idx_path)
    with open(meta_path, "rb") as f:
        meta_local = pickle.load(f)
    metas_local = meta_local["children"]

    print(f"üîß Using index dim={index_local.d} | Model={model.get_sentence_embedding_dimension()} | Domain={domain}")

    # Compute query embedding
    qv = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
    assert qv.shape[1] == index_local.d, (
        f"‚ùå Dimension mismatch: Query={qv.shape[1]} vs Index={index_local.d}"
    )

    # Retrieve top candidates
    D, I = index_local.search(qv, top_k * 5)
    picked = []
    for j, i in enumerate(I[0]):
        if i < len(metas_local):
            picked.append({
                **metas_local[i],
                "text": metas_local[i].get("text", ""),
                "faiss_score": float(D[0][j]),
            })

    ranked = rerank_bge(query, picked, top_k=top_k)
    return ranked, "Domain" if domain else "General"


def show_results(query: str, top_k: int = 5):
    print(f"\nüîé Query: {query}")
    for domain_flag in (False, True):
        results, mode = faiss_search(query, top_k=top_k, domain=domain_flag)
        print(f"\n=== {mode} Retrieval ===")
        for i, r in enumerate(results, 1):
            print(
                f"{i}. parent={r['parent_id'][-8:]} | gran={r['meta'].get('granularity','?')} | "
                f"FAISS={r.get('faiss_score',0):.3f} | Rerank={r.get('rerank_score','-')}"
            )
            print("   ", r["text"][:200].replace("\n", " "), "...\n")


# ---- Test again ----
show_results("factors leading to food spoilage in milk", top_k=5)

import pandas as pd

rows = []
for q in [
    "factors leading to food spoilage in milk",
    "oxidative rancidity in edible oils",
    "functions of vitamin A in maintaining vision",
    "methods used for food preservation at home"
]:
    g_hits, _ = faiss_search(q, top_k=3, domain=False)
    d_hits, _ = faiss_search(q, top_k=3, domain=True)
    g_mean = np.mean([h.get("rerank_score", 0) for h in g_hits])
    d_mean = np.mean([h.get("rerank_score", 0) for h in d_hits])
    rows.append({"Query": q, "General_AvgScore": g_mean, "Domain_AvgScore": d_mean})

pd.DataFrame(rows)

# ====================================================
# üìñ Enhanced Result Display ‚Äî Shows Full Retrieved Answers
# ====================================================

def show_results(query: str, top_k: int = 5, display_mode: str = "medium"):
    """
    Print retrieval results (FAISS + Rerank) for both General and Domain indexes.

    Args:
        query (str): search query
        top_k (int): number of top results to display
        display_mode (str): 'short', 'medium', or 'full' for answer length
    """
    print(f"\nüîé Query: {query}")
    print("=" * 100)

    for domain_flag in (False, True):
        results, mode = faiss_search(query, top_k=top_k, domain=domain_flag)

        print(f"\n=== {mode} Retrieval ===")
        print("-" * 100)

        for i, r in enumerate(results, 1):
            # Choose how much text to print
            text = r["text"]
            if display_mode == "short":
                text = text[:200]
            elif display_mode == "medium":
                text = text[:600]
            else:
                text = text[:1500]

            print(f"üîπ Rank {i}")
            print(f"üìÑ Source: {r['meta'].get('source', 'N/A')}")
            print(f"üìë Granularity: {r['meta'].get('granularity', '?')}")
            print(f"üß© Parent ID: {r['parent_id'][-8:]}")
            print(f"üìä FAISS Score: {r.get('faiss_score', 0):.3f}")
            print(f"üèÖ Reranker Score: {r.get('rerank_score', 0):.3f}")
            print(f"\nüìù Retrieved Text:\n{text}")
            print("-" * 100)

    print("\n‚úÖ Retrieval completed for both indexes.\n")

# Short summary view (first 200 chars)
show_results("factors leading to food spoilage in milk", top_k=3, display_mode="short")

# Medium detail (first 600 chars) ‚Äî good for reports
show_results("oxidative rancidity in edible oils", top_k=3, display_mode="medium")

# Full text (up to 1500 chars per answer) ‚Äî for debugging / detailed inspection
show_results("procedures for detecting adulteration in milk", top_k=3, display_mode="full")

"""# Elasticsearch path ‚Äî index (parent‚Äìchild) + query + rerank"""

!pip install -q elasticsearch

!pip install -q rank_bm25 faiss-cpu sentence-transformers tqdm

from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
import faiss, numpy as np
from tqdm.auto import tqdm

texts = [c["text"] for c in bundle["children"]]
metas = [c for c in bundle["children"]]

# BM25 setup
tokenized_corpus = [t.split() for t in texts]
bm25 = BM25Okapi(tokenized_corpus)

# FAISS setup (you already have a model loaded)
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
embs = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)
dim = embs.shape[1]
index = faiss.IndexFlatIP(dim)
index.add(embs)

def hybrid_search(query, top_k=5, alpha=0.5):
    """alpha balances BM25 and semantic score"""
    q_emb = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
    D, I = index.search(q_emb, len(texts))
    faiss_scores = 1 - D[0]                      # convert similarity‚Üídistance if needed
    bm25_scores = bm25.get_scores(query.split())

    # normalize and combine
    faiss_norm = (faiss_scores - np.min(faiss_scores)) / (np.max(faiss_scores) - np.min(faiss_scores) + 1e-8)
    bm25_norm  = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + 1e-8)
    hybrid = alpha * (1 - faiss_norm) + (1 - alpha) * bm25_norm

    ranked_idx = np.argsort(hybrid)[::-1][:top_k]
    return [(texts[i], metas[i], hybrid[i]) for i in ranked_idx]

query = "oxidative rancidity in edible oils"
for i, (t, m, s) in enumerate(hybrid_search(query, top_k=5, alpha=0.6), 1):
    print(f"\n{i}. Score={s:.3f} | Source={m['meta'].get('source','N/A')}\n{t[:300]}...")

